{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from data.cifar import CIFAR10NEW, CIFAR100\n",
    "from data.mnist import MNIST\n",
    "#from model import CNN\n",
    "import argparse, sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import shutil\n",
    "#from net_10 import Net\n",
    "#import PreResNet_3\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "# \t\t\tnn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
    "#             nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     70,
     93
    ]
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        n_classes=2,\n",
    "        depth=5,\n",
    "        wf=6,\n",
    "        padding=True,\n",
    "        batch_norm=True,\n",
    "        up_mode='upconv',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "        self.last = nn.Conv2d(prev_channels, 3, kernel_size=1)\n",
    "        self.out = F.sigmoid\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "        feature = x\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        outputlast = self.last(x)\n",
    "        output = self.out(outputlast)\n",
    "        return feature, output\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.Dropout2d(p=0.15)) # edited\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     7,
     11,
     36,
     62,
     90,
     119
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = conv3x3(3,64)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lin=0, lout=5):\n",
    "        out = x\n",
    "        if lin < 1 and lout > -1:\n",
    "            out = self.conv1(out)\n",
    "            out = self.bn1(out)\n",
    "            out = F.relu(out)\n",
    "        if lin < 2 and lout > 0:\n",
    "            out = self.layer1(out)\n",
    "        if lin < 3 and lout > 1:\n",
    "            out = self.layer2(out)\n",
    "        if lin < 4 and lout > 2:\n",
    "            out = self.layer3(out)\n",
    "        if lin < 5 and lout > 3:\n",
    "            out = self.layer4(out)\n",
    "        if lout > 4:\n",
    "            out = F.avg_pool2d(out, 4)\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(PreActBlock, [2,2,2,2], num_classes=num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3,4,6,3], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes=num_classes)\n",
    "\n",
    "def ResNet101(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,23,3], num_classes=num_classes)\n",
    "\n",
    "def ResNet152(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,8,36,3], num_classes=num_classes)\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(Variable(torch.randn(1,3,32,32)))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    #transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "train_dataset = CIFAR10NEW(root='./data/',\n",
    "                                    download=True,\n",
    "                                    train=True,\n",
    "                                    transform=transform_train,\n",
    "                                    noise_type='symmetric',\n",
    "                                    noise_rate=0.5\n",
    "                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4,\n",
    "                                               pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_dataset.train_data\n",
    "x = x.transpose(0,3,1,2)\n",
    "x = x/255\n",
    "for i in range(len(x)):\n",
    "    for j in range(x.shape[1]):\n",
    "        x[i][j] = (x[i][j] - mean[j]) / std[j]\n",
    "x[25972]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (data, target, index) in enumerate(train_loader):\n",
    "    print(data,index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdx(ref_meta, ref_batch, num_class):\n",
    "    tmp = np.zeros((len(ref_batch), num_class))\n",
    "    ref_meta = ref_meta.detach().cpu().numpy()\n",
    "    ref_batch = ref_batch.detach().cpu().numpy()\n",
    "    #print(ref_meta.shape,ref_batch.shape)\n",
    "    #print(ref_meta[0][1], ref_meta[1][1], ref_meta[2][1])\n",
    "    #print(\"hello\")\n",
    "    for i in range(len(ref_batch)):\n",
    "        for j in range(num_class):\n",
    "            #print(ref_batch[i], ref_meta[j])\n",
    "            #print(np.sum(ref_batch[i]*ref_meta[j], dtype=np.float),(np.linalg.norm(ref_batch[i])),(np.linalg.norm(ref_meta[j])))\n",
    "            tmp[i][j] = (np.sum(ref_batch[i]*ref_meta[j], dtype=np.float)/(np.linalg.norm(ref_batch[i]))/(np.linalg.norm(ref_meta[j])))\n",
    "    #res = np.zeros((len(ref_batch)))\n",
    "    #print(tmp)\n",
    "    res = np.argmax(tmp, axis=1)\n",
    "    return res\n",
    "\n",
    "def getIdx1(ref_meta, ref_batch, num_class):\n",
    "    tmp = np.zeros((len(ref_batch), num_class))\n",
    "    ref_meta = ref_meta.detach().cpu().numpy()\n",
    "    ref_batch = ref_batch.detach().cpu().numpy()\n",
    "    for i in range(len(ref_batch)):\n",
    "        for j in range(num_class):\n",
    "            tmp[i][j] = (np.sum(ref_batch[i]*ref_meta[j], dtype=np.float)/(np.linalg.norm(ref_batch[i]))/(np.linalg.norm(ref_meta[j])))\n",
    "    #res = np.argmax(tmp, axis=1)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\")# if use_cuda else \"cpu\")\n",
    "encoder = torch.load('./model_UNet1').to(device)\n",
    "encoder.eval()\n",
    "metadata = train_dataset.metadata\n",
    "metadata = metadata.transpose(0,3,1,2)  \n",
    "metadata = metadata/255\n",
    "for i in range(len(metadata)):\n",
    "    for j in range(metadata.shape[1]):\n",
    "        metadata[i][j] = (metadata[i][j] - mean[j]) / std[j] #image = (image - mean) / std\n",
    "metadata = torch.from_numpy(metadata).float().to(device)\n",
    "metadata = metadata.detach()\n",
    "metatarget = torch.from_numpy(train_dataset.metatarget).float().to(device)\n",
    "distance_true=[]\n",
    "distance_false=[]\n",
    "for batch_idx, (data, target, index, true_target) in enumerate(train_loader):\n",
    "    data, target, true_target = data.to(device), target.to(device), true_target.to(device)\n",
    "    print(batch_idx)\n",
    "    print(metatarget)\n",
    "    #optimizer.zero_grad()\n",
    "    #output = model(data)\n",
    "    #print(output.shape,data.shape)\n",
    "    #print(metadata.shape, data.shape)\n",
    "    ref_meta, _ = encoder(metadata)\n",
    "    ref_batch, _ = encoder(data)\n",
    "    #print(ref_meta.shape, len(ref_batch))\n",
    "    Idx = getIdx1(ref_meta, ref_batch, 10)\n",
    "    #print(Idx)\n",
    "    #Idx = torch.from_numpy(Idx).float().to(device)\n",
    "    #dijige = np.argmax(Idx)\n",
    "    if target == true_target:\n",
    "        distance_true.append(max(Idx[0]) - Idx[0][target])\n",
    "    else:\n",
    "        distance_false.append(max(Idx[0]) - Idx[0][target])\n",
    "    #output = F.log_softmax(output,dim=1)\n",
    "    #pseudotarget = output.argmax(dim=1)\n",
    "    #print(Idx, true_target)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "plt.hist(distance_true, bins=100, range=[0,1], density=None)\n",
    "plt.hist(distance_false, bins=100, range=[0,1], density=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainUNet(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _,output = model(data)\n",
    "        #print(output.shape,data.shape)\n",
    "        loss = nn.MSELoss()(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _,output = model(data)\n",
    "        #print(output.shape,data.shape)\n",
    "        loss = torch.sqrt(nn.MSELoss()(output, data))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def mainUNet():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    \n",
    "    parser.add_argument('--result_dir', type = str, help = 'dir to save result txt files', default = 'results/')\n",
    "    parser.add_argument('--noise_rate', type = float, help = 'corruption rate, should be less than 1', default = 0.5)\n",
    "    parser.add_argument('--forget_rate', type = float, help = 'forget rate', default = None)\n",
    "    parser.add_argument('--noise_type', type = str, help='[pairflip, symmetric]', default='symmetric')\n",
    "    parser.add_argument('--num_gradual', type = int, default = 10, help='how many epochs for linear drop rate, can be 5, 10, 15. This parameter is equal to Tk for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--exponent', type = float, default = 1, help='exponent of the forget rate, can be 0.5, 1, 2. This parameter is equal to c in Tc for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--top_bn', action='store_true')\n",
    "    parser.add_argument('--dataset', type = str, help = 'mnist, cifar10, or cifar100', default = 'cifar10')\n",
    "    parser.add_argument('--n_epoch', type=int, default=300)\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--print_freq', type=int, default=10)\n",
    "    parser.add_argument('--num_workers', type=int, default=2, help='how many subprocesses to use for data loading')\n",
    "    parser.add_argument('--num_iter_per_epoch', type=int, default=400)\n",
    "    parser.add_argument('--epoch_decay_start', type=int, default=80)\n",
    "    parser.add_argument('--eps', type=float, default=9.9)\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 256)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=4000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--noise-level', type=float, default=80.0,\n",
    "                        help='percentage of noise added to the data (values from 0. to 100.), default: 80.')\n",
    "    parser.add_argument('--root-dir', type=str, default='/home/iedl/w00536717/data', help='path to CIFAR dir where cifar-10-batches-py/ and cifar-100-python/ are located. If the datasets are not downloaded, they will automatically be and extracted to this path, default: .')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    batch_size=args.batch_size\n",
    "    \n",
    "\n",
    "#     if args.dataset=='cifar10':\n",
    "#         input_channel=3\n",
    "#         num_classes=10\n",
    "#         args.top_bn = False\n",
    "#         args.epoch_decay_start = 80\n",
    "#         args.n_epoch = 200\n",
    "#         train_dataset = CIFAR10NEW(root='./data/',\n",
    "#                                     download=True,\n",
    "#                                     train=True,\n",
    "#                                     transform=transforms.ToTensor(),\n",
    "#                                     noise_type='clean',\n",
    "#                                     noise_rate=args.noise_rate\n",
    "#                                )\n",
    "    \n",
    "#         test_dataset = CIFAR10NEW(root='./data/',\n",
    "#                                     download=True,\n",
    "#                                     train=False,\n",
    "#                                     transform=transforms.ToTensor(),\n",
    "#                                     noise_type='clean',\n",
    "#                                     noise_rate=args.noise_rate\n",
    "#                               )\n",
    "#     #\n",
    "#     if args.dataset=='cifar100':\n",
    "#         input_channel=3\n",
    "#         num_classes=100\n",
    "#         args.top_bn = False\n",
    "#         args.epoch_decay_start = 100\n",
    "#         args.n_epoch = 200\n",
    "#         train_dataset = CIFAR100(root='./data/',\n",
    "#                                     download=True,\n",
    "#                                     train=True,\n",
    "#                                     transform=transforms.ToTensor(),\n",
    "#                                     noise_type=args.noise_type,\n",
    "#                                     noise_rate=args.noise_rate\n",
    "#                                 )\n",
    "    \n",
    "#         test_dataset = CIFAR100(root='./data/',\n",
    "#                                     download=True,\n",
    "#                                     train=False,\n",
    "#                                     transform=transforms.ToTensor(),\n",
    "#                                     noise_type=args.noise_type,\n",
    "#                                     noise_rate=args.noise_rate\n",
    "#                                 )\n",
    "    # if args.forget_rate is None:\n",
    "    #     forget_rate=args.noise_rate\n",
    "    # else:\n",
    "    #     forget_rate=args.forget_rate\n",
    "    #\n",
    "    # noise_or_not = train_dataset.noise_or_not\n",
    "    # # Data Loader (Input Pipeline)\n",
    "    # print('loading dataset...')\n",
    "    # train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "    #                                            batch_size=batch_size,\n",
    "    #                                            num_workers=args.num_workers,\n",
    "    #                                            drop_last=True,\n",
    "    #                                            shuffle=True)\n",
    "    #\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "    #                                           batch_size=batch_size,\n",
    "    #                                           num_workers=args.num_workers,\n",
    "    #                                           drop_last=True,\n",
    "    #                                           shuffle=False)\n",
    "\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    if args.dataset == 'cifar10':\n",
    "        trainset = CIFAR10NEW(root=args.root_dir, train=True, download=True, transform=transform_train, noise_type='clean', noise_rate=args.noise_rate)\n",
    "        num_classes = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        trainset = CIFAR100(root=args.root_dir, train=True, download=True, transform=transform_train, noise_type='clean', noise_rate=args.noise_rate)\n",
    "        num_classes = 100\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=16,\n",
    "                                               pin_memory=True)\n",
    "    #train_loader_track = torch.utils.data.DataLoader(trainset_track, batch_size=args.batch_size, shuffle=False,\n",
    "     #                                                num_workers=4, pin_memory=True)\n",
    "    #test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=4,\n",
    "     #                                         pin_memory=True)\n",
    "\n",
    "    #labels = get_data_cifar_2(train_loader_track)  # it should be \"clonning\"\n",
    "    #noisy_labels = add_noise_cifar_wo(train_loader, args.noise_level,\n",
    "    #                                 args.noise_type)  # it changes the labels in the train loader directly\n",
    "    #noisy_labels_track = add_noise_cifar_wo(train_loader_track, args.noise_level, args.noise_type)\n",
    "\n",
    "    # Define models\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")# if use_cuda else \"cpu\")\n",
    "    cnn = UNet().to(device)\n",
    "    #cnn = nn.DataParallel(cnn,device_ids=[0,1,2,3])\n",
    "    #cnn = PreResNet_two.ResNet18(num_classes=10).to(device)\n",
    "    cnn.cuda()\n",
    "    #print(model.parameters)\n",
    "    #optimizer1 = torch.optim.SGD(cnn1.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=args.lr,weight_decay=1e-5,momentum=args.momentum)\n",
    "    #optimizer = torch.optim.Adam(cnn.parameters(), lr=args.lr)\n",
    "    #optimizer1 = torch.optim.SGD(cnn.parameters(), lr=1e-2,weight_decay=1e-4,momentum=args.momentum)\n",
    "    bmm_model = bmm_model_maxLoss = bmm_model_minLoss=0\n",
    "\n",
    "    acc=[]\n",
    "    loss=[]\n",
    "    loss_pure=[]\n",
    "    loss_corrupt=[]\n",
    "    out=[]\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        if epoch<200:\n",
    "            l1=trainUNet(args, cnn, device, train_loader, optimizer, epoch)\n",
    "            #acc.append(test(args, cnn, device, test_loader))\n",
    "    torch.save(cnn, './model_UNet1')\n",
    "    name=str(args.dataset)+\" \"+str(args.noise_type)+\" \"+str(args.noise_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    \n",
    "    parser.add_argument('--result_dir', type = str, help = 'dir to save result txt files', default = 'results/')\n",
    "    parser.add_argument('--noise_rate', type = float, help = 'corruption rate, should be less than 1', default = 0.5)\n",
    "    parser.add_argument('--forget_rate', type = float, help = 'forget rate', default = None)\n",
    "    parser.add_argument('--noise_type', type = str, help='[pairflip, symmetric]', default='symmetric')\n",
    "    parser.add_argument('--num_gradual', type = int, default = 10, help='how many epochs for linear drop rate, can be 5, 10, 15. This parameter is equal to Tk for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--exponent', type = float, default = 1, help='exponent of the forget rate, can be 0.5, 1, 2. This parameter is equal to c in Tc for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--top_bn', action='store_true')\n",
    "    parser.add_argument('--dataset', type = str, help = 'mnist, cifar10, or cifar100', default = 'cifar10')\n",
    "    parser.add_argument('--n_epoch', type=int, default=300)\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--print_freq', type=int, default=50)\n",
    "    parser.add_argument('--num_workers', type=int, default=2, help='how many subprocesses to use for data loading')\n",
    "    parser.add_argument('--num_iter_per_epoch', type=int, default=400)\n",
    "    parser.add_argument('--epoch_decay_start', type=int, default=80)\n",
    "    parser.add_argument('--eps', type=float, default=9.9)\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                        help='input batch size for training (default: 256)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=4000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    parser.add_argument('--noise-level', type=float, default=80.0,\n",
    "                        help='percentage of noise added to the data (values from 0. to 100.), default: 80.')\n",
    "    parser.add_argument('--root-dir', type=str, default='/home/iedl/w00536717/data', help='path to CIFAR dir where cifar-10-batches-py/ and cifar-100-python/ are located. If the datasets are not downloaded, they will automatically be and extracted to this path, default: .')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    batch_size=args.batch_size\n",
    "    \n",
    "\n",
    "    if args.dataset=='cifar10':\n",
    "        input_channel=3\n",
    "        num_classes=10\n",
    "        args.top_bn = False\n",
    "        args.epoch_decay_start = 80\n",
    "        args.n_epoch = 1\n",
    "        train_dataset = CIFAR10NEW(root='./data/',\n",
    "                                    download=True,\n",
    "                                    train=True,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    noise_type=args.noise_type,\n",
    "                                    noise_rate=args.noise_rate\n",
    "                               )\n",
    "    \n",
    "        test_dataset = CIFAR10NEW(root='./data/',\n",
    "                                    download=True,\n",
    "                                    train=False,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    noise_type=args.noise_type,\n",
    "                                    noise_rate=args.noise_rate\n",
    "                              )\n",
    "    #\n",
    "    if args.dataset=='cifar100':\n",
    "        input_channel=3\n",
    "        num_classes=100\n",
    "        args.top_bn = False\n",
    "        args.epoch_decay_start = 100\n",
    "        args.n_epoch = 200\n",
    "        train_dataset = CIFAR100(root='./data/',\n",
    "                                    download=True,\n",
    "                                    train=True,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    noise_type=args.noise_type,\n",
    "                                    noise_rate=args.noise_rate\n",
    "                                )\n",
    "    \n",
    "        test_dataset = CIFAR100(root='./data/',\n",
    "                                    download=True,\n",
    "                                    train=False,\n",
    "                                    transform=transforms.ToTensor(),\n",
    "                                    noise_type=args.noise_type,\n",
    "                                    noise_rate=args.noise_rate\n",
    "                                )\n",
    "    # if args.forget_rate is None:\n",
    "    #     forget_rate=args.noise_rate\n",
    "    # else:\n",
    "    #     forget_rate=args.forget_rate\n",
    "    #\n",
    "    # noise_or_not = train_dataset.noise_or_not\n",
    "    # # Data Loader (Input Pipeline)\n",
    "    # print('loading dataset...')\n",
    "    # train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "    #                                            batch_size=batch_size,\n",
    "    #                                            num_workers=args.num_workers,\n",
    "    #                                            drop_last=True,\n",
    "    #                                            shuffle=True)\n",
    "    #\n",
    "    # test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "    #                                           batch_size=batch_size,\n",
    "    #                                           num_workers=args.num_workers,\n",
    "    #                                           drop_last=True,\n",
    "    #                                           shuffle=False)\n",
    "\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    if args.dataset == 'cifar10':\n",
    "        trainset = datasets.CIFAR10(root=args.root_dir, train=True, download=True, transform=transform_train)\n",
    "        trainset_track = datasets.CIFAR10(root=args.root_dir, train=True, transform=transform_train)\n",
    "        testset = datasets.CIFAR10(root=args.root_dir, train=False, transform=transform_test)\n",
    "        num_classes = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        trainset = datasets.CIFAR100(root=args.root_dir, train=True, download=True, transform=transform_train)\n",
    "        trainset_track = datasets.CIFAR100(root=args.root_dir, train=True, transform=transform_train)\n",
    "        testset = datasets.CIFAR100(root=args.root_dir, train=False, transform=transform_test)\n",
    "        num_classes = 100\n",
    "    \n",
    "    metadata = train_dataset.metadata\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=16,\n",
    "                                               pin_memory=True)\n",
    "    #train_loader_track = torch.utils.data.DataLoader(trainset_track, batch_size=args.batch_size, shuffle=False,\n",
    "     #                                                num_workers=4, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=args.test_batch_size, shuffle=False, num_workers=16,\n",
    "                                             pin_memory=True)\n",
    "\n",
    "    #labels = get_data_cifar_2(train_loader_track)  # it should be \"clonning\"\n",
    "    #noisy_labels = add_noise_cifar_wo(train_loader, args.noise_level,\n",
    "    #                                 args.noise_type)  # it changes the labels in the train loader directly\n",
    "    #noisy_labels_track = add_noise_cifar_wo(train_loader_track, args.noise_level, args.noise_type)\n",
    "\n",
    "    # Define models\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")# if use_cuda else \"cpu\")\n",
    "    encoder = torch.load('./model_UNet')\n",
    "    cnn = ResNet18(num_classes).to(device)\n",
    "    #cnn = nn.DataParallel(cnn,device_ids=[0,1,2,3])\n",
    "    #cnn = cnn.to(device)\n",
    "    #cnn = PreResNet_two.ResNet18(num_classes=10).to(device)\n",
    "    cnn.cuda()\n",
    "    #print(model.parameters)\n",
    "    #optimizer1 = torch.optim.SGD(cnn1.parameters(), lr=learning_rate)\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=args.lr,weight_decay=1e-5,momentum=args.momentum)\n",
    "    #optimizer = torch.optim.Adam(cnn.parameters(), lr=args.lr)\n",
    "    #optimizer1 = torch.optim.SGD(cnn.parameters(), lr=1e-2,weight_decay=1e-4,momentum=args.momentum)\n",
    "    #bmm_model = bmm_model_maxLoss = bmm_model_minLoss=0\n",
    "\n",
    "    acc=[]\n",
    "    loss=[]\n",
    "    loss_pure=[]\n",
    "    loss_corrupt=[]\n",
    "    out=[]\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        if epoch<200:\n",
    "            l1=train(args, cnn, encoder, device, train_loader, optimizer, epoch)\n",
    "            acc.append(test(args, cnn, device, test_loader))\n",
    "    torch.save(cnn, './model_UNet')\n",
    "    name=str(args.dataset)+\" \"+str(args.noise_type)+\" \"+str(args.noise_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) \n",
    "sys.argv = ['-f']\n",
    "mainUNet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36-torch",
   "language": "python",
   "name": "py36-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
